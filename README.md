# TISA

This is the code for the article The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models

Please cite this article as follows:
```
@inproceedings{wennberg-henter-2021-case,
    title = "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models",
    author = "Wennberg, Ulme  and
      Henter, Gustav Eje",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.18",
    doi = "10.18653/v1/2021.acl-short.18",
    pages = "130--140"
}
```

## Model

The TISA model is located in `model/tisa.py`

## Figures

### Bert base uncased
<img src="figures/EpEpT/bert-base-uncased.png" alt="bert-base-uncased" class="inline"/>

### Bert large uncased
<img src="figures/EpEpT/bert-large-uncased.png" alt="bert-large-uncased" class="inline"/>

### Bert base cased
<img src="figures/EpEpT/bert-base-cased.png" alt="bert-base-cased" class="inline"/>

### Bert large cased
<img src="figures/EpEpT/bert-large-cased.png" alt="bert-large-cased" class="inline"/>

### Roberta base
<img src="figures/EpEpT/roberta-base.png" alt="roberta-base" class="inline"/>

### Roberta large
<img src="figures/EpEpT/roberta-large.png" alt="roberta-large" class="inline"/>

### Albert base v1
<img src="figures/EpEpT/albert-base-v1.png" alt="albert-base-v1" class="inline"/>

### Albert base v2
<img src="figures/EpEpT/albert-base-v2.png" alt="albert-base-v2" class="inline"/>

### Albert large v1
<img src="figures/EpEpT/albert-large-v1.png" alt="albert-large-v1" class="inline"/>

### Albert large v2
<img src="figures/EpEpT/albert-large-v2.png" alt="albert-large-v2" class="inline"/>

### Albert xlarge v1
<img src="figures/EpEpT/albert-xlarge-v1.png" alt="albert-xlarge-v1" class="inline"/>

### Albert xlarge v2
<img src="figures/EpEpT/albert-xlarge-v2.png" alt="albert-xlarge-v2" class="inline"/>

### Albert xxlarge v1
<img src="figures/EpEpT/albert-xxlarge-v1.png" alt="albert-xxlarge-v1" class="inline"/>

### Albert xxlarge v2
<img src="figures/EpEpT/albert-xxlarge-v2.png" alt="albert-xxlarge-v2" class="inline"/>

### Longformer base
<img src="figures/EpEpT/allenai_longformer-base-4096.png" alt="allenai_longformer-base-4096" class="inline"/>

### Longformer base
<img src="figures/EpEpT/allenai_longformer-large-4096.png" alt="allenai_longformer-large-4096" class="inline"/>

### KB Bert base swedish cased
<img src="figures/EpEpT/KB_bert-base-swedish-cased.png" alt="KB_bert-base-swedish-cased" class="inline"/>

### KB Albert base swedish cased alpha
<img src="figures/EpEpT/KB_albert-base-swedish-cased-alpha.png" alt="KB_albert-base-swedish-cased-alpha" class="inline"/>


